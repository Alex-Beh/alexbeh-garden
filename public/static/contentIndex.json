{"Latex":{"title":"Latex","links":[],"tags":[],"content":"Cheatsheet §\nUseful cheatsheet that I found from Steven Gong: http://tug.ctan.org/info/undergradmath/undergradmath.pdf"},"Libary/Ceres":{"title":"Ceres","links":[],"tags":[],"content":"https://blog.csdn.net/LDST_CSDN/article/details/130674774"},"Libary/Eigen":{"title":"Eigen","links":[],"tags":[],"content":"API §\n\nEigen::Matrix&lt;double, 2, 3&gt;: Matrix4f is a 2x3 matrix of doubles\n\n[a1​b1​​a2​b2​​a3​b3​​]\n\n\n\nUsing block operations §\nEigen::MatrixXfm(4,4)=​15913​261014​371115​481216​​(1)\n\nm.leftCols(2)\n\n​15913​261014​​\n\n\nm.bottomRows&lt;2&gt;()\n[913​1014​1115​1216​]\n\n\n\nColumn-major and row-major storage §\n\n\nRow-major\nMatrix is stored in row-major order if it is stored row by row. The entire first row is stored first, followed by the entire second row, and so on.\nIf this matrix(1) is stored in row-major order, then the entries are laid out in memory as follows:\n1 2 3 4 5 6 ... 15 16\n\n\nColumn-major\nIf it is stored in col-major order,  it is laid out as follows: 1 5 9 13 2 6 ... 12 16\n\n"},"Linear-Algebra/Column-Space":{"title":"Column Space","links":["Linear-Algebra/Subspaces"],"tags":[],"content":"Given a matrix A, all the linear combinations of the columns of A form a Subspaces. This is the column space C(A).\nFor example, if A=​0−11​100​​, the column space of A is the plane through the origin in R3 containing ​0−11​​ and ​100​​.\n\nColumn space of A and solving Ax=b §\nGiven a matrix A, the system of linear equations Ax=b is solvable exactly when b is a vector in the column space of A.\nNullspace of A §\nThe Nullspace* of a matrix A is the collection x to the equation Ax=0\nFor example,\n​1234​1111​2345​​​x1​x2​x3​​​=​0000​​,\nthe Nullspace N(A) consists of all multiples of ​11−1​​. This Nullspace is a line in R3.\nAttention, a nullspace is a vector space because it obviously satisfies the requirements about addition and scale multiplication. That is to say that any sum or multiple of solutions of Ax=0 is also a solution: A(x1​+x2​)=0+0=0 and A(cx)=cAx=c0=0.\nOther values of b §\nThe solutions to the equation:\n​1234​1111​2345​​​x1​x2​x3​​​=​1234​​,\ndo not form a subspace. This conclusion is obvious since the zero vector is not a solution to the equation. Actually, the set of solutions forms a line in R3 that pass through the points ​100​​ and ​0−11​​, but not ​000​​."},"Linear-Algebra/Elimination-of-Matrices":{"title":"Elimination of Matrices","links":["Linear-Algebra/Matrix-Type"],"tags":[],"content":"We have an example Ax=b,\nA=​130​284​111​​ and b=​2122​​.\nSteps of Elimination:\n\nStep 1: subtract 3 times row 1 from row 2;\nStep 2: subtract 2 times row 2 from row 3.\n\nA=​130​284​111​​→​100​224​1−21​​→U=​100​210​1−25​​\nb=​2122​​→⋯→​26−10​​\n\nFrom row3, 5z=−10, so z=−2\n\nThus, we can easily solve the systems of equations, ​xyz​​=​21−2​​.\nElimination Matrices §\nThe product of a matrix (3x3) and a column vector (3x1) is a column vector (3x1) that is a linear combination of the columns of the matrix.\nThe product of a row vector (1x3) and a matrix (3x3) is a row vector (1x3) that is a linear combination of the rows of the matrix.\nFor example,\n​1−30​010​001​​​130​284​111​​=​100​224​1−21​​.\nMultiplying on the left by a permutation matrix exchanges the rows of a matrix, while multiplying on the right exchanges the columns. For example,\nP=​010​100​001​​.\nP is a Permutation Matrix and the first and second rows of the matrix PA are the second and first rows of the matrix A.\nNote, matrix multiplication is associative but not commutative."},"Linear-Algebra/Factorization-into-A-=-LU":{"title":"Factorization into A = LU","links":[],"tags":[],"content":""},"Linear-Algebra/Gauss-Jordan-Elimination":{"title":"Gauss-Jordan Elimination","links":[],"tags":[],"content":"E[A​I​]=[I​E​]\nIf EA=I, then E=A−1."},"Linear-Algebra/Introduction-to-Linear-Algebra":{"title":"Introduction to Linear Algebra","links":[],"tags":[],"content":"Vectors §\nClosure §\nA collection of vectors has to satisfy two conditions:\n\nclosed under addition, which means the sum of any two vectors in the collection lies again in the collection,\nclosed under multiplication by any real numbers, that is to say that multiplying any vector in the collection by any real number will not give a vector beyond the collection,\nor, to put it in another way, closed under linear combinations.\ns.t. we call the collection a vector space.\n\nMatrix §\nPivot Point §\n\nAssociative\n矩阵乘法虽然不能随意变动相乘次序，但是可以变动括号位置\nCommutative\n\nCondition to solve Ax=b or Ax=0 §\n\n\nA is invertible\n\nAx=b has the unique solution x for each b\nAx=0 has no non-zero solution x\nThe columns of A are independent\nAll vectors Ax cover the whole vector space \nExample: A=​1−10​01−1​001​​\n\n\n\nA is not invertible\n\nAx=b has a solution x only for some of b in the vector space\nAx=0 has non-zero solutions x\nThe columns of A are dependent\nAll vectors Ax lies in only a subspace of the vector space \nExample: A=​1−10​01−1​−101​​\n\n\n"},"Linear-Algebra/Matrix-Multiplication":{"title":"Matrix Multiplication","links":[],"tags":[],"content":"Blocks §\n[A1​A3​​A2​A4​​][B1​B3​​B2​B4​​]=[A1​B1​+A2​B3​A3​B1​+A4​B3​​A1​B2​+A2​B4​A3​B2​+A4​B4​​]\nLinear Combination §\nA matrix times by a vector is just a linear combination of the column vectors of the matrix.\n[2−1​−12​][12​]=1[2−1​]+2[−12​]=[03​]​"},"Linear-Algebra/Matrix-Spaces":{"title":"Matrix Spaces","links":["Linear-Algebra/Matrix-Type","Linear-Algebra/Subspaces","Linear-Algebra/Rank,-Basis,-Dimension"],"tags":[],"content":"3 by 3 matrices §\nWe identified M, the space of all 3 by 3 matrices; S, the space of all symmetric 3 by 3 matrices; U, the space of all upper triangular 3 by 3 matrices and D, the space of all diagonal 3 by 3 matrices.\nYou can see the introduction of Matrix Type\nThe dimension of M is 9. A good choice of basis is:\n​100​000​000​​,​000​100​000​​,​000​000​100​​,…,​000​001​000​​,​000​000​001​​.\nThe dimension of S is 6 and one basis is:\n​100​000​000​​,​010​100​000​​,​001​000​100​​,​000​010​000​​,​000​001​010​​,​000​000​001​​.\nThe dimension of U is also 6 and a basis for U is:\n​100​000​000​​,​000​100​000​​,​000​000​100​​,​000​010​000​​,​000​000​010​​,​000​000​001​​.\nThe subspace D=S∩U has dimension 3. A good basis is:\n​100​000​000​​,​000​010​000​​,​000​000​001​​.\nS∪U is NOT a Subspaces of M. However, S+U, which means all possible sums of elements of S and elements of U, is a subspace of M. In fact, the subspace is just M itself. We can find that dimensions follow this rule:\ndim S+dim U=dim (S+U)+dim (S∩U).\nDifferential equations §\nWe can think of the solutions y to dx2d2y​+y=0 as the elements of a Nullspace.\nThe complete solution is:\ny=c1​cosx+c2​sinx,\nwhere c1​ and c2​ can be any complex numbers. The solution space is a two dimensional vector space with Basis vectors cosx and sinx."},"Linear-Algebra/Matrix-Type":{"title":"Matrix Type","links":["Math/Skew-Symmetric-Matrix"],"tags":[],"content":"\nIdentity Matrix\nUpper Triangular Matrix\nLower Triangular Matrix\nSymmetric Matrix \nDiagonal Matrix\n[[#Skew Symmetric Matrix|Skew Symmetric Matrix]]\nSquare Matrix\n\nPermutation Matrix §\nA permutation matrix  has rows of the identity in any order.\nEvery row and every column of a permutation matrix contain exactly one nonzero entry,\nwhich is 1. There are two 2 × 2 permutation matrices:\n[10​01​]&amp;[01​10​]\nEvery permutation matrix is a product of elementary row-interchange matrices.\n[01​10​][x1​x2​​]=[x2​x1​​]\nIdentity Matrix §\n​10⋮0​01⋮0​……⋱…​00⋮1​​\nUpper Triangular Matrix §\n​a11​00⋮0​a12​a22​0⋮0​a13​a23​a33​⋮0​………⋱…​a1n​a2n​a3n​⋮ann​​​\nLower Triangular Matrix §\n​a11​a21​a31​⋮an1​​0a22​a32​⋮an2​​00a33​⋮an3​​………⋱…​000⋮ann​​​\nSymmetric Matrix §\nA=AT\n​a11​a12​a13​​a12​a22​a23​​a13​a23​a33​​​\nDiagonal Matrix §\n​d1​0⋮0​0d2​⋮0​……⋱…​00⋮dn​​​\nSkew Symmetric Matrix §\n​0a12​−a13​​−a12​0a23​​a13​−a23​0​​\nSquare Matrix §\nNumber of row = Number of column\n​a11​a21​⋮an1​​a12​a22​⋮an2​​……⋱…​a1n​a2n​⋮ann​​​"},"Linear-Algebra/Multiplication-and-Inverse-Matrices":{"title":"Multiplication and Inverse Matrices","links":[],"tags":[],"content":"We have AB=C. A is an m×n matrix and B is an n×p matrix, then C is an m×p matrix. We use cij​ to denote the entry in row i and column j of matrix C and the same denotation applies to aij​ and bij​.\nRow times column §\ncij​=∑k=1n​aik​bkj​\nColumns §\nThe product of matrix A and column j of matrix B equals column j of matrix C. This tells us that the columns of C are combinations of columns of A.\nA​∣column1∣​∣column2∣​∣column3∣​​=​∣A(column1)∣​∣A(column2)∣​∣A(column3)∣​​\nRows §\nThe product of row i of matrix A and matrix B equals row i of matrix C. So the rows of C are combinations of rows of B.\n​−−−−−−−−−​row1row2row3​−−−−−−−−−​​B=​−−−−−−−−−​(row1)B(row2)B(row3)B​−−−−−−−−−​​\nColumn times row §\nAB=k=1∑​n​a1k​⋮amk​​​[bk1​​⋯​bkp​​]\nBlocks §\n[A1​A3​​A2​A4​​][B1​B3​​B2​B4​​]=[A1​B1​+A2​B3​A3​B1​+A4​B3​​A1​B2​+A2​B4​A3​B2​+A4​B4​​]\nInverses §\nIf A is singular or not invertible,\nthen A does not have an inverse,\nand we can find some non-zero vector x for which Ax=0\nGauss-Jordan Elimination §\nE[A​I​]=[I​E​]\nIf EA=I, then E=A−1."},"Linear-Algebra/Null-Space":{"title":"Null Space","links":[],"tags":[],"content":"The null space of an m × n matrix A, written as NUL(A), is the set of all solutions to the homogeneous equation Ax = 0."},"Linear-Algebra/Rank-1-Matrix":{"title":"Rank 1 Matrix","links":["Linear-Algebra/Rank,-Basis,-Dimension"],"tags":[],"content":"\nRank = Dimension\n\nA=[12​48​510​]=[12​][1​4​5​].\nA has Rank 1 because each of its columns is a multiple of the first column.\nEvery rank 1 matrix A can be written A=uvT, where u and v are column vectors. We can use rank 1 matrices as building blocks for more complex matrices."},"Linear-Algebra/Rank,-Basis,-Dimension":{"title":"Rank, Basis, Dimension","links":[],"tags":[],"content":"In full rank matrices, or r = m = n\n\nThe matrix must be square\nThere is one unique solution to every b.\nThe reduced-row echelon form R is the identity I.\nThere is nothing in the null space\nThe matrix is invertible\n\nIn full column rank matrices, or r = n &lt; m\n\nThere is 1 or 0 solutions to every b\nThe reduced-row echelon form R is the identity I on top of a zero matrix\nThere is nothing in the null space\n\nIn full row rank matrices, or r = m &lt; n\n\nThere is an infinite amount of solutions to every b.\nThe reduced-row echelon form R is the identity I to the left of a zero matrix.\nThere is n-r special solutions in the null space.\n\n\nRank §\nThe rank of the matrix represents the amount of independent columns in the matrix.\nWe then realize that the amount of pivot columns is the rank, since pivot columns = independent columns.\nFurthermore, we can also get the amount of free columns in a (m, n) matrix by doing n - r, which gives us the amount of free columns.\nSo far:\n\nr is the amount of independent / pivot columns, as well as pivot variables\nn - r is the amount of dependent / free columns, as well as free variables\n\nAnd in extension,\n\nn - r is the amount of vectors that define the null space\nr is the amount of vectors that define the column space\n\nFull Rank; r = m = n §\nObviously this is a square matrix and every columns is linearly independent.\nThey have no entries in their null space except for the zero vector {0}.\nFull Column Rank; r = n, r &lt; m §\nThe difference is that you have dependent rows, or leftover rows.\n\nThus, when we cancel out, we get the equivalent system Ux=c.\n\nNow Ux=c  can only be solved if the two solvability conditions are true. only if b3–2b1​=0 and b4–2b2​=0 can it be true.\nBut if those two conditions are true, we are left with a single answer to solve our system. That is why we say that with a r = n (full column rank) matrix, we either have 0 or 1 solutions. 0 if one or more of our solvability conditions is false, and 1 if all solvability conditions are true.\nFull Row Rank; r = m, r &lt; n §\nIn this scenario, our matrix does have free variables and free columns, and thus has entries in the null space.\n\nThe second column is the first column x 2, and the fourth column is the second column x 2. Thus, when we cancel out, we can get the updated equation Ux=c.\n\nSince our answer will be two dimensional, and we have the basis vectors to describe two dimensional space in the first two columns of our matrix, we can solve any answer b.\nBut more importantly than that, we also have our ‘free columns’ of zeros. These multiply our ‘free variables’ of z and t. Thus, we can set z and t to any constants we like, since they will multiply by 0. From this, we can say that we have an infinite amount of solutions to any answer.\nBasis §\nIn mathematics, a set B of vectors in a vector space V is called a basis if every element of V may be written in a unique way as a finite linear combination of elements of B.\n\nThe smallest set of vectors that generates the space\n\n\nDimension §\nAs a general rule, rank = dimension, or r = dimension."},"Linear-Algebra/SVD":{"title":"SVD","links":[],"tags":[],"content":"The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal.\nAnxp​= Unxn​∗Snxp​∗VpxpT​\nhttps://medium.com/@doublekien/singular-value-decomposition-svd-e01ef6291604"},"Linear-Algebra/Solving-Ax-=-0-->-Pivot-Variables,-Special-Solutions":{"title":"Solving Ax = 0 -> Pivot Variables, Special Solutions","links":[],"tags":[],"content":""},"Linear-Algebra/Solving-Ax-=-b-->-Reduced-Row-Form-R":{"title":"Solving Ax = b -> Reduced Row Form R","links":[],"tags":[],"content":""},"Linear-Algebra/Spans":{"title":"Spans","links":[],"tags":[],"content":"In mathematics, the linear span of a set S of vectors (from a vector space), denoted span(S), is defined as the set of all linear combinations of the vectors in S.\n\na is span of b and c if a=k1​b+k2​c where k1​ and k2​ are constants.\n\n\n"},"Linear-Algebra/Subspaces":{"title":"Subspaces","links":["Linear-Algebra/Matrix-Multiplication"],"tags":[],"content":"We say that a subset U of a vector space V is a subspace of V if U is a vector space under the inherited addition and scalar multiplication operations of V.\n\nthe origin,\na line through the origin,\na plane through the origin,\nall of R3.\n\nConsider a plane P in ℜ3 through the origin:\nax+by+cz(a​b​c​)∗​xyz​​MX​=0=0=0​\n\nIf X1​ and X2​ are both solutions to MX=0, then by linearity of Matrix Multiplication, so is μX1​+υX2​:\nM(μX1​+υX2​)=μMX1​+υMX2​=0"},"Linear-Algebra/The-Four-Fundamental-Subspaces":{"title":"The Four Fundamental Subspaces","links":["Linear-Algebra/Subspaces"],"tags":[],"content":"\nFour subspaces and their basis and dimension\n[[#column-space-ca|Column space C(A)]]\n[[#nullspace-na|Nullspace N(A)]]\n[[#row-space-camathrmt|Row space C(AT)]]\n[[#left-nullspace-namathrmt|Left Nullspace N(AT)]]\n- [[#how-to-find-a-basis-for-namathrmt|how to find a basis for N(AT)]]\nNew vector space\n\nFour subspaces and their basis and dimension §\nAny m by n matrix A determines four Subspaces (possibly containing only the zero vector):\nColumn space C(A) §\nAll combinations of the columns of A\nA subspace of Rm\nThe r pivot columns form a basis\ndim C(A)=r\nNullspace N(A) §\nAll solutions x of the equation Ax=0\nA subspace of Rn\nThe n−r special solutions from a basis\ndim N(A)=n−r\nRow space C(AT) §\nAll combinations of the rows of  A\nA subspace of Rn\nThe  first r rows of  R (the reduced row echelon form of A) form a basis\ndim C(AT)=r\nLeft Nullspace N(AT) §\nAll solutions y of the equation yTA=0\nA subspace of Rm\nThe bottom m−r of E form a basis\ndim N(AT)=m−r\nhow to find a basis for N(AT) §\nFirst we get E through Gauss-Jordan elimination:\nEm×n​[Am×n​​Im×n​​]=[Rm×n​​Em×n​​]\nThen the bottom m−r rows of E describe linear dependencies of rows of A since the bottom m−r rows of R are zero. For example:\nEA=​−111​2−10​001​​​111​212​323​111​​=​100​010​110​100​​=R\nIn this example, The last row of E satisfy the equation yTA=0 and thus form a basis for the left Nullspace of A.\nNew vector space §\nWe put all 3 by 3 matrices together and see the  collection as a new vector space; we call it M.\nSome subspace of M include:\n\nall upper triangular matrices\nall symmetric matrices\nD, all diagonal matrices\n\nD is the intersection of the first two space. It also has a dimension of 3 and a basis for D is:\n​100​000​000​​,​100​030​000​​,​000​000​007​​."},"Linear-Algebra/The-Geometry-of-Linear-Equations":{"title":"The Geometry of Linear Equations","links":[],"tags":[],"content":"We have a system of equations:\n{2x−y−x+2y​=0=3​\nRow Picture §\nLine 2x−y=0 and line −x+2y=0 intersects at the point (1,2), so (1,2) is the solution of the system of equations.\nColumn Picture §\nWe rewrite the system of linear equations as a single equation:\nx[2−1​]+y[−12​]=[03​]\nWe see x and y as scalars of column vectors: v1​=[2−1​] and v2​=[−12​], and the sum xv1​+yv2​ is called a linear combination of v1​ and v2​.\nGeometrically, we can find one copy of v1​ added to two copies of v2​ just equals the vector [03​]. Then the solution should be x=1,y=2.\nMatrix Picture §\nWe rewrite the equations in our example as a compact form,\nAx=b,\nthat is\n[2−1​−12​][xy​]=[03​]"},"Linear-Algebra/Transposes":{"title":"Transposes","links":["Linear-Algebra/Matrix-Type"],"tags":[],"content":"(AT)ij​=Aji​\nGiven any matrix R the product RTR is always Symmetric Matrix, which means the transpose of a matrix equals itself, because (RTR)T=RT(RT)T=RTR."},"Math/Cross-Product":{"title":"Cross Product","links":[],"tags":[],"content":"Cross product is a binary operation on two vectors in three-dimensional space. It results in a vector that is perpendicular to both vectors. Vector products are also called cross products.\nc=a×b​a1​a2​a3​​​×​b1​b2​b3​​​​=​ia1​b1​​ja2​b2​​ka3​b3​​​=​a2​b3​−a3​b2​a3​b1​−a1​b3​a1​b2​−a2​b1​​​​\n \nUse case §\nArea of Parallelogram §\n∣x×y​∣=∣x∣∣y​∣sinθ\n\nVolume of Parallelepiped §\nA parallelepiped is the three dimensional analogue of a parallelogram.\nV=∣w⋅(x×y​)∣\n"},"Math/Singular-Value-decomposition":{"title":"Singular Value decomposition","links":[],"tags":[],"content":""},"Math/Skew-Symmetric-Matrix":{"title":"Skew Symmetric Matrix","links":["Math/Cross-Product"],"tags":[],"content":"A skew-symmetric matrix A is a square matrix whose transpose equals its negative, i,e. AT=−A\na∧=​0a3​−a2​−a3​0a1​​a2​−a1​0​​\nThe main use case is it can simplify the Cross Product between two matrix into dot product.\na×b=a∧⋅b​​\nProof §\nLet’s assume a=[a1​​a2​​a3​​]T and b=[b1​​b2​​b3​​]T\na×b​=​a2​b3​−a3​b2​a3​b1​−a1​b3​a1​b2​−a2​b1​​​=​0a3​−a2​−a3​0a1​​a2​−a1​0​​⋅​b1​b2​b3​​​=a∧⋅b​"},"Ubuntu/Chrony":{"title":"Chrony","links":[],"tags":[],"content":"Overview §\n\n\nstart &amp; stop chrony service\n\n\ncheck status\nsudo systemctl status chronyd.service\n# show last 20 lines\njournalctl -n 20 -u chrony\n\n\n\nRun chrony manually with custom command [mostly for sync]\nsudo chronyd -d &#039;server 10.252.252.100 iburst&#039;\n\n\n\nchronyc tracking §\nExpected output:\nadministrator@A31-003324021:~$ chronyc tracking\nReference ID    : 0AFCFC64 (10.252.252.100)\nStratum         : 5\nRef time (UTC)  : Tue Sep 12 12:21:26 2023\nSystem time     : 0.000000102 seconds slow of NTP time\nLast offset     : +0.000109387 seconds\nRMS offset      : 0.000109387 seconds\nFrequency       : 78.816 ppm fast\nResidual freq   : +0.002 ppm\nSkew            : 1.982 ppm\nRoot delay      : 0.086848132 seconds\nRoot dispersion : 0.025397623 seconds\nUpdate interval : 64.3 seconds\nLeap status     : Normal\n\nchronyc sources -v §\n查看ntp_servers\nMS Name/IP address         Stratum Poll Reach LastRx Last sample               \n===============================================================================\n^* 10.252.252.100                4   6    77    39   -231us[ -388us] +/-   66ms\n\nSample Configuration §\nServer §\npool ntp.ubuntu.com        iburst maxsources 4\npool 0.ubuntu.pool.ntp.org iburst maxsources 1\npool 1.ubuntu.pool.ntp.org iburst maxsources 1\npool 2.ubuntu.pool.ntp.org iburst maxsources 2\n\n# This directive specify the location of the file containing ID/key pairs for\n# NTP authentication.\nkeyfile /etc/chrony/chrony.keys\n\n# This directive specify the file into which chronyd will store the rate\n# information.\ndriftfile /var/lib/chrony/chrony.drift\n\n# Uncomment the following line to turn logging on.\n#log tracking measurements statistics\n\n# Log files location.\nlogdir /var/log/chrony\n\n# Stop bad estimates upsetting machine clock.\nmaxupdateskew 100.0\n\n# This directive enables kernel synchronisation (every 11 minutes) of the\n# real-time clock. Note that it can’t be used along with the &#039;rtcfile&#039; directive.\nrtcsync\n\n# Step the system clock instead of slewing it if the adjustment is larger than\n# one second, but only in the first three clock updates.\nmakestep 1 3\n\n# Configure this host to act as the NTP source for the rest of the devices in this robot\nlocal stratum 10\nallow 192.168.131/24\n\nClient §\npool ntp.ubuntu.com        iburst maxsources 4\npool 0.ubuntu.pool.ntp.org iburst maxsources 1\npool 1.ubuntu.pool.ntp.org iburst maxsources 1\npool 2.ubuntu.pool.ntp.org iburst maxsources 2\n\n# Add the robot&#039;s primary PC as an NTP source\n#server 10.252.252.100 offline minpoll 8\nserver 10.252.252.100 iburst\n\n# This directive specify the location of the file containing ID/key pairs for\n# NTP authentication.\nkeyfile /etc/chrony/chrony.keys\n\n# This directive specify the file into which chronyd will store the rate\n# information.\ndriftfile /var/lib/chrony/chrony.drift\n\n# Uncomment the following line to turn logging on.\n#log tracking measurements statistics\n\n# Log files location.\nlogdir /var/log/chrony\n\n# Stop bad estimates upsetting machine clock.\nmaxupdateskew 100.0\n\n# This directive enables kernel synchronisation (every 11 minutes) of the\n# real-time clock. Note that it can’t be used along with the &#039;rtcfile&#039; directive.\nrtcsync\n\n# Step the system clock instead of slewing it if the adjustment is larger than\n# one second, but only in the first three clock updates.\nmakestep 1 3\n\nReference: Configuring NTP"},"Vslam/Group":{"title":"Group","links":[],"tags":[],"content":""},"Vslam/Homogeneous-Coordinates":{"title":"Homogeneous Coordinates","links":[],"tags":[],"content":""},"Vslam/Lie-Group":{"title":"Lie Group","links":["Vslam/Group"],"tags":[],"content":"Group"},"Vslam/Transformation-Notation":{"title":"Transformation Notation","links":[],"tags":[],"content":""},"Vslam/Triangulation":{"title":"Triangulation","links":["Math/Cross-Product","Math/Skew-Symmetric-Matrix","Math/Singular-Value-decomposition","Vslam/Homogeneous-Coordinates"],"tags":[],"content":"To find out the feature point in world coordinate when it is observed at least in two camera frames\nGiven:\n\n2D image observations [u,v] in multiple frames.\nCamera intrinsic matrix K.\nCamera poses [Rci​w​,tci​w​] for each observation frame.\n\n\nTheory §\n​xyz​​​=s1​∗[Rci​w​,tci​w​]∗Pw​​xyz​​​=K−1∗​uv1​​​(1)​\nHomogeneous Method §\n\nFrom Equation (1)\n\n​xyz​​​=s1​∗​Tci​w​1∗PTci​w​2∗PTci​w​3∗P​​​\n\nDo the Cross Product on both side\n\n0=​xyz​​×​xyz​​=​xyz​​×​Tci​w​1∗PTci​w​2∗PTci​w​3∗P​​\n\nTransform into Skew Symmetric Matrix\n\n​−Tci​w​2PTci​w​1P−yTci​w​1P​+yTci​w​3P−xTci​w​3P+xTci​w​3P​​​=​000​​(2)\n\nSince row3 is linearly dependant on row1 &amp; row2, so we need another set of Equation(2) in order to recover the 3D Position of the feature point.\n\n\nWe can solve the Equation(2) by using Singular Value decomposition. Since P is Homogeneous Coordinates, so we need to homogeneous V by P=V[0:3]/V[4].\nWhen there is noise in the measurement, V[4] could be a value that is close to zero. Then the recovered position from P might be a infinity point. This is a problem of doing triangulation by homogeneous method.\n\nNon-Homogeneous Method §\nCode Implementation §\nHomogeneous Method §\n"},"Vslam/Visual-Factor-in-VINS-Fusion":{"title":"Visual Factor in VINS-Fusion","links":["Vslam/Lie-Group"],"tags":[],"content":"projectionOneFrameTwoCamFactor §\nParameters to be optimised:\n\n[Pbi​w​,qbi​w​] : SE(3) pose of body in frame i\n[Pbj​w​,qbj​w​] : SE(3) pose of body in frame j\n[pcb​,qcb​]: Extrinsic parameter between body and camera\nλl​: inverse depth, it is the depth of feature in frame i\ntd​: time difference between camera &amp; imu\n\n\nresidualsrc​​=measurement−result_from_projection=[uj​−Zj​Xj​​vj​−Zj​Yj​​​]​\nresidual = (pts_camera_j / dep_j).head&lt;2&gt;() - pts_j_td.head&lt;2&gt;();\nresidual = sqrt_info * residual;\nThere are 4 Jacobian in Evaluate function.\n\n\nFor feature point that is first observed in camera at frame i and project it into camera at frame j\n​Xcj​​Ycj​​Zcj​​1​​=Tbc​∗Twbj​​∗Tbi​w​∗Tcb​∗​Xci​​Yci​​Zci​​1​​(1)\nSince λ1​=Zci​​ and using the projection function, let’s further modify Equation(1)\n​Xcj​​Ycj​​Zcj​​1​​=Tbc​∗Twbj​​∗Tbi​w​∗Tcb​∗​λ1​uci​​λ1​vci​​λ1​1​​(2)\nExpand the Equation(2) by Tba​Pb=Rba​Pb+tba​,\nfcj​​=​Xcj​​Ycj​​Zcj​​​​​=Rbc​Rwbj​​Rbi​w​Rcb​λ1​​uci​​vci​​1​​+Rbc​(Rbj​w​(Rbi​w​pcb​+pbi​w​−pbj​w​)−pcb​)​(3)\nWhat is our target? To find out the derivative of residual, which is the Jacobian of Lie Group\n\nBy using Chain Rule:\nδPδrc​​=δfcj​​δrc​​∗δPδfcj​​​(4)\nP consists of all the parameters that we want to optimise:\n[note: td​ is missing in the picture]\n\n\n\n​\t\tFor the first term in Equation(4), in code it is declare as reduce\nδfcj​​δrc​​=​Zcj​​1​0​0Zcj​​1​​−Zcj​2​Xcj​​​−Zcj​2​Ycj​​​​​(5)\n​\t\tFor the second term in Equation(4), let’s take the derivative for each parameters\n\n\nJacobian[0]\nδ[δpbi​w​δθbiw​​​]δfcj​​​=[δpbi​w​δfcj​​​,δθbi​w​δfcj​​​,​]\nLet’s take derivative example for the first term,\nδpbi​w​δfcj​​​=Rbc​Rwbj​​\n\n\nJacobian[1]\nEigen::Matrix&lt;double, 3, 6&gt; jaco_j;\njaco_j.leftCols&lt;3&gt;() = ric.transpose() * -Rj.transpose();\njaco_j.rightCols&lt;3&gt;() = ric.transpose() * Utility::skewSymmetric(pts_imu_j);\njaco_j3x6​=​∣∣∣​−δpbi​w​δfcj​​​−​∣∣∣​∣∣∣​−δqbi​w​δfcj​​​−​∣∣∣​​\n\n\nJacobian[2]: extrinsic parameter between body and camera\n\n\nJacobian[3]: about feature\n∂λl​∂rC​​where ∂λl​∂plcj​​​​=∂plcj​​∂rC​​∂λl​∂plcj​​​=Rbc​Rwbj​​Rbi​w​Rcb​πc−1​([ulci​​vlci​​​])λl2​1​​\n\n\nJacobian[4]: about time td​\n\n\nReference §\n\n\nVINS-MONO理论学习---紧耦合后端非线性优化\n\n\nVINS 系统中的视觉重投影因子\n\n"},"achieve/Graph":{"title":"Graph","links":["tags/edges","tags/nodes"],"tags":["edges","nodes"],"content":"Graph consist of Nodes &amp; Edges where m(row) is#edges &amp; n(col) is#nodes\nFor the following example, the graph is consist of 4 nodes and 5 edges.\n\nIncidence Matrices §\nedge1​edge2​edge3​edge4​edge5​​node1​−10−1−10​node2​1−1000​node3​0110−1​node4​00011​​\nWrite into Matrix form\nA=​−10−1−10​1−1000​0110−1​00011​​\nObservation from the first three rows, we notice there is linear dependent relationship, which mean the three row spaces form an loop."},"index":{"title":"Welcome to my Second Brain","links":[],"tags":[],"content":"I am Alex Beh, a Robotics Engineer from Singapore.\nSee the documentation for how to get started."}}